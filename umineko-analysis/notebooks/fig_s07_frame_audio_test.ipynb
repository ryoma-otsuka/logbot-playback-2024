{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Difference Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tck\n",
    "import seaborn as sns\n",
    "palette2 = sns.color_palette([\"#D81B60\", \"#1E88E5\", \"#FFC107\", \"#004D40\"])\n",
    "palette = palette2\n",
    "sns.set_theme(context='poster', style='ticks', palette=palette, font_scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"C:/Users/ryoma/D/logbot-data/umineko/Umineko2024/v5-umineko-2024-playback/v5-vid-mic-zure-test\"\n",
    "# Change the path appropriately\n",
    "# base_dir = \"../path_to_base_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_audio = pd.read_csv(f\"{base_dir}/experiment-data/clap_start_audio.csv\")\n",
    "df_frame = pd.read_csv(f\"{base_dir}/experiment-data/clap_start_frame.csv\")\n",
    "# print(len(df_audio))\n",
    "# print(len(df_frame))\n",
    "\n",
    "df = pd.merge(df_audio, df_frame, on='id', how='left', suffixes=('', '_right'))\n",
    "df.insert(len(df.columns), 'time_diff', df['peak_time_ms'] - df['clap_start_ms'])\n",
    "print(f\"N = {len(df)}\")\n",
    "print(f\"time diff mean: {np.mean(df['time_diff']):.2f}\")\n",
    "print(f\"time diff median: {np.median(df['time_diff']):.2f}\")\n",
    "print(f\"time diff maximum: {np.max(df['time_diff']):.2f}\")\n",
    "print(f\"time diff min: {np.min(df['time_diff']):.2f}\")\n",
    "print(f\"time diff range: {(np.max(df['time_diff']) - np.min(df['time_diff'])):.2f}\")\n",
    "print(f\"time diff var: {np.var(df['time_diff']):.2f}\")\n",
    "print(f\"time diff std: {np.std(df['time_diff']):.2f}\")\n",
    "print(f\"time diff 25% quantile: {np.quantile(df['time_diff'], q=0.25):.2f}\")\n",
    "print(f\"time diff 75% quantile: {np.quantile(df['time_diff'], q=0.75):.2f}\")\n",
    "# display(df)\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(558)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "median = np.median(df['time_diff'])\n",
    "min = np.min(df['time_diff'])\n",
    "max = np.max(df['time_diff'])\n",
    "\n",
    "ax.axhline(xmin=0, xmax=10, y=max, color=palette[0], linestyle='-',    label=f'max      = {max:.2f}')\n",
    "ax.axhline(xmin=0, xmax=10, y=median, color=palette[1], linestyle=\"-\", label=f'median = {median:.2f}')\n",
    "ax.axhline(xmin=0, xmax=10, y=min, color=palette[2], linestyle='-',    label=f'min       = {min:.2f}')\n",
    "\n",
    "ax.yaxis.set_minor_locator(tck.MultipleLocator(25))\n",
    "ax.grid(which='minor', axis='y', linestyle='-', linewidth='0.75')\n",
    "ax.grid(which='major', axis='y', linestyle='-', linewidth='0.75')\n",
    "\n",
    "# Violin plot\n",
    "violin_parts = sns.violinplot(\n",
    "    x='device', y='time_diff', data=df, \n",
    "    inner=None,\n",
    "    linewidth=0.0, color='#333333', saturation=0.5,\n",
    "    ax=ax\n",
    ")\n",
    "for pc in violin_parts.collections:\n",
    "    pc.set_edgecolor('black')\n",
    "    pc.set_facecolor('#333333')\n",
    "    pc.set_alpha(0.25)\n",
    "\n",
    "# Box plot\n",
    "unique_devices = df['device'].unique()\n",
    "device_positions = np.arange(len(unique_devices)) + 0.25  # shift 0.25 to right\n",
    "data_by_device = [df[df['device'] == device]['time_diff'] for device in unique_devices]\n",
    "\n",
    "box_parts = ax.boxplot(\n",
    "    data_by_device,\n",
    "    positions=device_positions,\n",
    "    widths=0.15,\n",
    "    patch_artist=True,\n",
    "    medianprops=dict(color='black', linewidth=3.0),\n",
    "    whiskerprops=dict(color='black', linewidth=2.0),\n",
    "    capprops=dict(color='black', linewidth=2.0),\n",
    "    boxprops=dict(facecolor='white', color='black', linewidth=2.0),\n",
    ")\n",
    "\n",
    "ax.set_xticks(np.arange(len(unique_devices)))\n",
    "ax.set_xticklabels(unique_devices)\n",
    "\n",
    "np.random.seed(558)\n",
    "sns.stripplot(x='device', y='time_diff', data=df, color=\"#333333\", jitter=True, size=9, alpha=0.7, ax=ax)\n",
    "ax.set_yticks(np.arange(0, 500, 50))\n",
    "ax.set_ylim(130, 330)\n",
    "\n",
    "plt.legend(ncol=1)\n",
    "plt.xlabel('Device', labelpad=10)\n",
    "plt.ylabel('Time difference (ms)', labelpad=10)\n",
    "plt.show()\n",
    "save_dir = \"../output/figure-for-paper/\"\n",
    "# fig.savefig(f\"{save_dir}/png/fig_s07_zure_video_audio_time_diff.png\", dpi=350, bbox_inches=\"tight\", pad_inches=0.25, transparent=False)\n",
    "# fig.savefig(f\"{save_dir}/pdf/fig_s07_zure_video_audio_time_diff.pdf\", dpi=600, bbox_inches=\"tight\", pad_inches=0.25, transparent=False)\n",
    "# fig.savefig(f\"{save_dir}/zure_video_audio_time_diff.svg\", bbox_inches=\"tight\", pad_inches=0.25, transparent=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1000 / 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Data Peak Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def road_wav_data_and_info(audio_path):\n",
    "    dirname = os.path.dirname(audio_path)\n",
    "    device = os.path.basename(dirname)\n",
    "    fname = os.path.basename(audio_path).replace(\".wav\", \"\")\n",
    "    sample_rate, data = wavfile.read(audio_path)\n",
    "    data = data / np.max(np.abs(data)) # normalization\n",
    "    \n",
    "    print(f\"audio_path: {audio_path}\")\n",
    "    print(f\"sample_rate: {sample_rate}\")\n",
    "    print(f\"len(data): {len(data)}\")\n",
    "    print(f\"length: {len(data)/sample_rate}\")\n",
    "\n",
    "    return sample_rate, data, dirname, device, fname\n",
    "\n",
    "def detect_clap_starts(sample_rate, data, threshold=0.5, distance=10000):\n",
    "    # Calculate energy\n",
    "    energy = data ** 2\n",
    "    # Detect peaks using scipy.signal's find_peaks function\n",
    "    peaks, _ = find_peaks(energy, height=threshold, distance=distance)\n",
    "    # Calculate peak times\n",
    "    peak_times = peaks / sample_rate\n",
    "    \n",
    "    return peak_times, energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raw_data_and_energy_with_peaks(sample_rate, data, energy, peak_times, sup_title=None):\n",
    "    GRIDSPEC_KW = {'wspace': 0.1, 'hspace': 0.5}\n",
    "    fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(20, 8), gridspec_kw=GRIDSPEC_KW)\n",
    "    \n",
    "    # ax0\n",
    "    ax0.plot(data, label='Data', color=\"#555555\")\n",
    "    ax0.set_yticks(np.arange(-2.0, 2.0, 0.5))\n",
    "    ax0.set_ylim(-1.1, 1.1)\n",
    "    ax0.set_ylabel(\"Amplitude\", labelpad=10)\n",
    "    ax0.legend()\n",
    "    ax0.grid()\n",
    "\n",
    "    # ax1\n",
    "    ax1.plot(energy, color=palette[1], label='Energy')\n",
    "    ax1.scatter(\n",
    "        peak_times * sample_rate, \n",
    "        energy[peak_times.astype(int) * sample_rate], \n",
    "        color=palette[0], zorder=5, label='Clap Starts'\n",
    "    )\n",
    "    ax1.set_yticks(np.arange(-1.0, 2.0, 0.5))\n",
    "    ax1.set_ylim(-0.1, 1.1)\n",
    "    # ax1.set_title(\"Clap Detection\", pad=10)\n",
    "    ax1.set_xlabel(\"Sample Index\")\n",
    "    ax1.set_ylabel(\"Energy\", labelpad=10)\n",
    "    ax1.legend()\n",
    "    ax1.grid()\n",
    "\n",
    "    if sup_title is not None:\n",
    "        fig.suptitle(sup_title)\n",
    "\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path_target = f'{base_dir}/experiment-data/*/*.wav'\n",
    "audio_path_list = sorted(glob.glob(audio_path_target))\n",
    "print(f\"N of wav files: {len(audio_path_list)}\")\n",
    "\n",
    "device_list = []\n",
    "fname_list = []\n",
    "clap_list = []\n",
    "peak_times_s_list = []\n",
    "peak_times_ms_list = []\n",
    "peak_times_kakeru_30fps_list = []\n",
    "peak_times_kakeru_30fps_hiku_1_list = []\n",
    "# for i, audio_path in enumerate(audio_path_list[:4]):\n",
    "for i, audio_path in enumerate(audio_path_list):\n",
    "    \n",
    "    # Road audio data\n",
    "    sample_rate, data, dirname, device, fname = road_wav_data_and_info(audio_path)\n",
    "\n",
    "    threshold = 0.1  #  \n",
    "    distance = 10000 # minimum distance between peaks\n",
    "    peak_times, energy = detect_clap_starts(sample_rate, data, threshold, distance)\n",
    "    print(f\"N of peaks: {len(peak_times)}\")\n",
    "    \n",
    "    # 検出された手拍子の開始時間を表示\n",
    "    # print(f'Detected clap start times (in seconds): {peak_times}')\n",
    "    # print(f'Detected clap start frame index:, {peak_times * 30 - 1}')\n",
    "    device_list.extend([device]*len(peak_times))\n",
    "    fname_list.extend([fname]*len(peak_times))\n",
    "    clap_list.extend(np.arange(1, len(peak_times)+1, 1))\n",
    "    peak_times_s_list.extend(peak_times)\n",
    "    peak_times_ms_list.extend(peak_times*1000)\n",
    "    peak_times_kakeru_30fps_list.extend(peak_times * 30)\n",
    "    peak_times_kakeru_30fps_hiku_1_list.extend(peak_times * 30 - 1)\n",
    "\n",
    "    # 可視化\n",
    "    sup_title = f\"{device}: {fname}\"\n",
    "    fig = plot_raw_data_and_energy_with_peaks(sample_rate, data, energy, peak_times, sup_title=sup_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'id': list(np.arange(1, len(device_list)+1, 1)),\n",
    "    'device': device_list,\n",
    "    'fname': fname_list,\n",
    "    'clap': clap_list,\n",
    "    'peak_time_s': peak_times_s_list,\n",
    "    'peak_time_ms': peak_times_ms_list,\n",
    "    'peak_index_1': peak_times_kakeru_30fps_list,\n",
    "    'peak_index_0': peak_times_kakeru_30fps_hiku_1_list,\n",
    "}\n",
    "df = pd.DataFrame(data_dict)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"{base_dir}/experiment-data/clap_start_audio.csv\"\n",
    "# df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 1\n",
    "In an uncorrected version of the video data:\n",
    "\n",
    "The sound is heard slightly after the moment when the hands clap together.\n",
    "This audio lag is inherent in this version of the video.\n",
    "The original video conversion software can correct this issue.  \n",
    "\n",
    "## Point 2\n",
    "Due to the short distance, the difference between the speed of light and the speed of sound should not be detectable.\n",
    "\n",
    "Speed of Light: Approximately 880,000 times faster than the speed of sound.  \n",
    "Speed of Sound: 340m/s.  \n",
    "Given the speed of light is so fast, its delay time can be ignored.\n",
    "  \n",
    "At a distance of 500 mm (50 cm):  \n",
    "\n",
    "$$t = \\frac{\\text{Distance}}{\\text{Speed of Sound}} = \\frac{0.5 \\, \\mathrm{m}}{340 \\, \\mathrm{m/s}} \\approx 1.46 \\, \\mathrm{ms}$$\n",
    "\n",
    "Theoretically, this slight delay does exist. However, it is at a level that humans cannot perceive. Moreover, when compared to the audio lag visible in the uncorrected video data, this theoretical delay is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umineko-analysis-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
